# Implementation Plan: RAG Content Pipeline

**Branch**: `011-rag-content-pipeline` | **Date**: 2025-12-30 | **Spec**: [spec.md](./spec.md)
**Input**: Feature specification from `specs/011-rag-content-pipeline/spec.md`

## Summary

This plan outlines the technical approach for building a Python-based ingestion pipeline. The pipeline will crawl a deployed Docusaurus website, extract and chunk the textual content, generate vector embeddings using the Cohere API, and store the results in a Qdrant Cloud vector database. The entire process will be managed as a runnable script, configured via environment variables.

## Technical Context

**Language/Version**: Python 3.11
**Primary Dependencies**: `cohere`, `qdrant-client`, `beautifulsoup4`, `requests`, `python-dotenv`, `uv`
**Storage**: Qdrant Cloud (Vector Database)
**Testing**: `pytest`
**Target Platform**: Any platform with a Python environment (script-based).
**Project Type**: Standalone script/backend process.
**Performance Goals**: Process the entire website content in a reasonable timeframe (e.g., under 1 hour).
**Constraints**: Must operate within the free tiers of Cohere and Qdrant Cloud. The script should handle network errors gracefully.
**Scale/Scope**: The pipeline will process a single website of up to a few hundred pages.

## Constitution Check

*GATE: Must pass before Phase 0 research. Re-check after Phase 1 design.*

- [x] **Spec-Driven**: All work is traceable to `specs/011-rag-content-pipeline/spec.md`.
- [x] **Grounding**: The pipeline is designed specifically to process the book's content for the RAG system.
- [x] **Modernity**: The stack uses Qdrant, aligning with the project's principles.
- [x] **Constraints**: The solution is designed as a script to be run in a CLI environment and respects the free-tier limitations by using Qdrant Cloud.

## Project Structure

### Documentation (this feature)

```text
specs/011-rag-content-pipeline/
├── plan.md              # This file
├── research.md          # Research on crawling, chunking, and tooling
├── data-model.md        # Schema for the Qdrant vector records
├── quickstart.md        # Setup and execution guide
├── contracts/           # Not applicable for this script-based feature
└── tasks.md             # To be generated by /sp.tasks
```

### Source Code (repository root)

```text
backend/
├── .env.example         # Example environment file
├── .venv/               # Managed by uv
├── main.py              # Main script for the ingestion pipeline
└── pyproject.toml       # Project metadata and dependencies for uv
```

**Structure Decision**: A single `backend/` directory is chosen because this feature is a self-contained script, not a complex web application. This simple structure is easy to navigate and manage.

## Phase 0: Research

Key technical decisions were researched and have been consolidated in the `research.md` document. The research covered website crawling, content extraction, text chunking strategies, and the selection of Python libraries.

- **Reference**: [research.md](./research.md)

## Phase 1: Design

### Data Model

The data model for the vector records to be stored in Qdrant is defined in `data-model.md`. It specifies the schema for the vector payload, including the source text, URL, and other metadata.

- **Reference**: [data-model.md](./data-model.md)

### API Contracts

Not applicable. This feature is a backend script and does not expose a public API.

### High-Level Architecture

The implementation will be a single Python script (`main.py`) orchestrated by a `main()` function. The script will be modular, with distinct functions for each stage of the pipeline:

1.  **Configuration**: Load credentials and settings from a `.env` file.
2.  **Crawling**: Fetch `/sitemap.xml` and gather all content URLs.
3.  **Processing Loop**: For each URL:
    a.  **Fetch**: Download the page's HTML.
    b.  **Extract**: Parse the HTML with BeautifulSoup to get the text from the `<main>` tag.
    c.  **Chunk**: Split the extracted text into smaller, overlapping chunks.
    d.  **Embed**: Send chunks to the Cohere API in batches to generate embeddings.
    e.  **Store**: Upsert the embeddings and their metadata payloads to Qdrant in batches.
4.  **Logging**: Log progress, successes, and errors to the console throughout the process.

### Quickstart Guide

A detailed guide for setting up the environment, installing dependencies with `uv`, and running the pipeline is available in `quickstart.md`.

- **Reference**: [quickstart.md](./quickstart.md)

## Complexity Tracking

No violations of the project constitution were identified. Complexity is low.