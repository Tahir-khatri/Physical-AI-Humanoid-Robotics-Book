"use strict";(globalThis.webpackChunkbook_frontend=globalThis.webpackChunkbook_frontend||[]).push([[392],{2027(e){e.exports=JSON.parse('{"tag":{"label":"Overview","permalink":"/Physical-AI-Humanoid-Robotics-Book/docs/tags/overview","allTagsPath":"/Physical-AI-Humanoid-Robotics-Book/docs/tags","count":4,"items":[{"id":"introduction/01-ros-2-overview","title":"ROS 2: The Robotic Nervous System","description":"Before a humanoid robot can take a single step, before it can process a single pixel from its camera, it needs a nervous system. In the world of modern robotics, that nervous system is the Robot Operating System (ROS), and specifically its second iteration, ROS 2. This chapter provides a high-level overview of ROS 2, establishing the foundational understanding of how a robot\'s disparate hardware and software components communicate to create a single, cohesive, and functional whole. It is the essential bedrock upon which all subsequent modules of this book\u2014from digital twins to advanced AI brains\u2014are built.","permalink":"/Physical-AI-Humanoid-Robotics-Book/docs/introduction/01-ros-2-overview"},{"id":"introduction/03-isaac-perception-overview","title":"The AI-Robot Brain: Isaac & Perception","description":"With a nervous system (ROS 2) and a virtual body (our Digital Twin) in place, we can now construct the core of our robot\'s autonomy the perception system. We will introduce the NVIDIA Isaac platform, a suite of tools specifically designed for building the high-performance, GPU-accelerated perception pipelines required by modern AI-powered robots. This is where raw sensor data transforms into meaningful understanding, enabling the robot to navigate, interact, and operate autonomously.","permalink":"/Physical-AI-Humanoid-Robotics-Book/docs/introduction/03-isaac-perception-overview"},{"id":"introduction/04-vla-capstone-overview","title":"The Convergence: VLA & Capstone","description":"We arrive at the final and most exciting stage of our journey: the convergence of all the systems we have built into a single, cognitive, autonomous whole. We have a nervous system (ROS 2), a virtual body (Digital Twin), and a hardware-accelerated perception system (Isaac ROS). Now, we will give our robot a true \\"mind\\" by integrating a Large Language Model (LLM) to act as a high-level cognitive planner. This is the domain of Vision-Language-Action (VLA) models, the frontier of Embodied AI research. This chapter provides a high-level overview of the concepts we will implement in Module 4, culminating in the capstone project. The goal is to build a system where you can give the robot a complex, natural language command like, \\"Clean the room,\\" and the robot can autonomously decompose that goal into a sequence of physical actions and execute them.","permalink":"/Physical-AI-Humanoid-Robotics-Book/docs/introduction/04-vla-capstone-overview"},{"id":"introduction/02-digital-twin-overview","title":"The Digital Twin: Simulation Foundations","description":"In the realm of digital AI, an algorithm can be tested billions of times in a fraction of a second. An error might cause a program to crash, but the consequences are contained within the computer. In robotics, the stakes are infinitely higher. An error in a walking algorithm can cause a multi-million-dollar humanoid to fall and break. An error in a navigation algorithm could cause the robot to collide with a person. Developing and testing directly on physical hardware is slow, expensive, and dangerous. This is why the second pillar of modern robotics is the Digital Twin\u2014a high-fidelity simulation that acts as a virtual proxy for the real robot in a virtual world.","permalink":"/Physical-AI-Humanoid-Robotics-Book/docs/introduction/02-digital-twin-overview"}],"unlisted":false}}')}}]);