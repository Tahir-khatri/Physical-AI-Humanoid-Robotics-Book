"use strict";(globalThis.webpackChunkbook_frontend=globalThis.webpackChunkbook_frontend||[]).push([[629],{8453(e,n,i){i.d(n,{R:()=>s,x:()=>a});var o=i(6540);const t={},r=o.createContext(t);function s(e){const n=o.useContext(r);return o.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:s(e.components),o.createElement(r.Provider,{value:n},e.children)}},9346(e,n,i){i.r(n),i.d(n,{assets:()=>c,contentTitle:()=>a,default:()=>h,frontMatter:()=>s,metadata:()=>o,toc:()=>l});const o=JSON.parse('{"id":"module-4/chapter-1","title":"Chapter 1: Voice-to-Action with OpenAI Whisper","description":"Welcome to the final module, where we give our robot its voice and a higher level of cognitive function. The ultimate goal of robotics is to create machines that can interact with humans and the world in a natural, intuitive way. The most natural form of human communication is speech. This chapter provides an exhaustive technical guide to building the first component of this system: a Voice-to-Action pipeline.","source":"@site/docs/module-4/chapter-1.md","sourceDirName":"module-4","slug":"/module-4/chapter-1","permalink":"/Physical-AI-Humanoid-Robotics-Book/docs/module-4/chapter-1","draft":false,"unlisted":false,"editUrl":"https://github.com/Tahir-khatri/Physical-AI-Humanoid-Robotics-Book/tree/main/docs/module-4/chapter-1.md","tags":[],"version":"current","frontMatter":{"id":"chapter-1","title":"Chapter 1: Voice-to-Action with OpenAI Whisper","sidebar_label":"Voice-to-Action with Whisper"},"sidebar":"bookSidebar","previous":{"title":"Nav2 & Bipedal Path Planning","permalink":"/Physical-AI-Humanoid-Robotics-Book/docs/module-3/chapter-3"},"next":{"title":"Cognitive Planning with LLMs","permalink":"/Physical-AI-Humanoid-Robotics-Book/docs/module-4/chapter-2"}}');var t=i(4848),r=i(8453);const s={id:"chapter-1",title:"Chapter 1: Voice-to-Action with OpenAI Whisper",sidebar_label:"Voice-to-Action with Whisper"},a="Chapter 1: Voice-to-Action with OpenAI Whisper",c={},l=[{value:"Why Whisper for Robotics?",id:"why-whisper-for-robotics",level:2},{value:"The Voice-to-Action Pipeline Architecture",id:"the-voice-to-action-pipeline-architecture",level:2},{value:"Building the Whisper Bridge Node",id:"building-the-whisper-bridge-node",level:2},{value:"1. Audio Data Handling",id:"1-audio-data-handling",level:3},{value:"2. Whisper Integration",id:"2-whisper-integration",level:3},{value:"Conceptual <code>whisper_ros_bridge.py</code>",id:"conceptual-whisper_ros_bridgepy",level:3},{value:"How to Use this Node",id:"how-to-use-this-node",level:3},{value:"From Text to Action: The Command Parser",id:"from-text-to-action-the-command-parser",level:2}];function d(e){const n={code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"chapter-1-voice-to-action-with-openai-whisper",children:"Chapter 1: Voice-to-Action with OpenAI Whisper"})}),"\n",(0,t.jsxs)(n.p,{children:["Welcome to the final module, where we give our robot its voice and a higher level of cognitive function. The ultimate goal of robotics is to create machines that can interact with humans and the world in a natural, intuitive way. The most natural form of human communication is speech. This chapter provides an exhaustive technical guide to building the first component of this system: a ",(0,t.jsx)(n.strong,{children:"Voice-to-Action pipeline"}),"."]}),"\n",(0,t.jsxs)(n.p,{children:["We will bridge the gap between human speech and robotic action by integrating ",(0,t.jsx)(n.strong,{children:"OpenAI's Whisper"}),", a state-of-the-art speech recognition model, into our ROS 2 ecosystem. We will create a robust pipeline that can listen to a microphone, transcribe spoken words into text, parse that text for commands, and trigger ROS 2 services as a result."]}),"\n",(0,t.jsx)(n.h2,{id:"why-whisper-for-robotics",children:"Why Whisper for Robotics?"}),"\n",(0,t.jsx)(n.p,{children:"In a robotics context, a speech recognition system needs to be more than just accurate; it needs to be robust to a variety of real-world conditions."}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Noise Robustness"}),": Robots are not operated in soundproof booths. They have noisy fans, whirring motors, and operate in environments with background chatter. Whisper has been trained on a massive and diverse dataset from the web, making it exceptionally good at understanding speech even in the presence of significant background noise."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Accuracy"}),': Whisper provides near-human-level accuracy on a wide range of accents, languages, and technical vocabularies. For a robot, the difference between "go to the ',(0,t.jsx)(n.em,{children:"chair"}),'" and "go to the ',(0,t.jsx)(n.em,{children:"stair"}),"\" is critical. Whisper's accuracy minimizes these potentially dangerous misinterpretations."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Open Source and Local Execution"}),": While a cloud API is available, Whisper's models are open source. This means you can download and run them locally on your own hardware (especially on NVIDIA GPUs for best performance). For robotics, local execution is a huge advantage as it reduces latency (no internet round-trip) and enhances privacy (sensitive audio data doesn't leave the robot)."]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"the-voice-to-action-pipeline-architecture",children:"The Voice-to-Action Pipeline Architecture"}),"\n",(0,t.jsx)(n.p,{children:"Our goal is to build a modular pipeline composed of distinct ROS 2 nodes, each with a single responsibility."}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Audio Capture Node"}),": A simple node that uses a library like ",(0,t.jsx)(n.code,{children:"sounddevice"})," in Python to capture raw audio data from a microphone and publish it to a ROS 2 topic."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Whisper Bridge Node (The Core of this Chapter)"}),": This is the central component. It subscribes to the raw audio topic, accumulates audio data, sends it to the Whisper model for transcription, and publishes the resulting text to another topic."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Command Parser Node"}),": This node subscribes to the text transcription topic. It contains simple logic (e.g., regular expressions or keyword spotting) to parse the text and determine if it contains a valid command for the robot."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Action Client Node(s)"}),": Once the Command Parser identifies a valid command, it doesn't execute the action itself. Instead, it calls a ROS 2 service or action server on the appropriate node (e.g., the ",(0,t.jsx)(n.code,{children:"nav2_bringup"})," action server) to execute the high-level behavior."]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"This decoupled architecture is robust and extensible. We can swap out the Whisper node for a different STT engine or create a more advanced NLP-based command parser without changing the other components of the system."}),"\n",(0,t.jsx)(n.h2,{id:"building-the-whisper-bridge-node",children:"Building the Whisper Bridge Node"}),"\n",(0,t.jsxs)(n.p,{children:["Let's focus on the ",(0,t.jsx)(n.code,{children:"whisper_ros_bridge.py"})," script. This node will perform several key functions."]}),"\n",(0,t.jsx)(n.h3,{id:"1-audio-data-handling",children:"1. Audio Data Handling"}),"\n",(0,t.jsxs)(n.p,{children:["The node will subscribe to a topic publishing ",(0,t.jsx)(n.code,{children:"audio_common_msgs/msg/AudioData"}),". It will need to buffer this incoming audio data. It's inefficient to run a Whisper transcription on every tiny chunk of audio. Instead, we'll accumulate audio for a few seconds or use a Voice Activity Detection (VAD) library (like ",(0,t.jsx)(n.code,{children:"webrtcvad"}),') to detect when a user has started and stopped speaking. For this guide, we\'ll use a simpler "push-to-talk" approach where we trigger transcription manually.']}),"\n",(0,t.jsx)(n.h3,{id:"2-whisper-integration",children:"2. Whisper Integration"}),"\n",(0,t.jsxs)(n.p,{children:["We'll use the official ",(0,t.jsx)(n.code,{children:"openai-whisper"})," Python library. You can install it via pip: ",(0,t.jsx)(n.code,{children:"pip install openai-whisper"}),". You will also need ",(0,t.jsx)(n.code,{children:"ffmpeg"})," installed on your system."]}),"\n",(0,t.jsxs)(n.p,{children:["The core of the integration involves loading the model and then calling the ",(0,t.jsx)(n.code,{children:"transcribe()"})," function on our accumulated audio data."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"import whisper\n\n# Load the model. 'base' is a good starting point. \n# For higher accuracy on a GPU, you might use 'medium' or 'large'.\nmodel = whisper.load_model(\"base\")\n\n# result is a dictionary containing the transcribed text and other info\nresult = model.transcribe(\"path/to/my/audio_file.wav\")\ntranscribed_text = result[\"text\"]\nprint(transcribed_text)\n"})}),"\n",(0,t.jsxs)(n.h3,{id:"conceptual-whisper_ros_bridgepy",children:["Conceptual ",(0,t.jsx)(n.code,{children:"whisper_ros_bridge.py"})]}),"\n",(0,t.jsx)(n.p,{children:"Here is a conceptual script for our node. It will expose a service to start/stop recording and trigger transcription."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nfrom std_srvs.srv import Trigger\nfrom std_msgs.msg import String\nimport sounddevice as sd\nimport numpy as np\nimport whisper\nfrom scipy.io.wavfile import write\n\nclass WhisperBridge(Node):\n    def __init__(self):\n        super().__init__('whisper_bridge')\n        \n        # ROS 2 Publishers and Services\n        self.transcription_publisher = self.create_publisher(String, '/voice_transcription', 10)\n        self.srv = self.create_service(Trigger, 'trigger_transcription', self.trigger_transcription_callback)\n        \n        # Whisper Model\n        self.get_logger().info(\"Loading Whisper model...\")\n        self.model = whisper.load_model(\"base.en\") # Using the English-only base model\n        self.get_logger().info(\"Whisper model loaded.\")\n        \n        # Audio recording parameters\n        self.samplerate = 16000  # 16kHz is standard for Whisper\n        self.channels = 1\n        self.audio_buffer = []\n        self.is_recording = False\n\n    def trigger_transcription_callback(self, request, response):\n        if not self.is_recording:\n            # Start recording\n            self.is_recording = True\n            self.audio_buffer = [] # Clear buffer\n            self.get_logger().info('Recording started...')\n            \n            # This is a simplified approach. A real implementation would use a separate thread.\n            # Here we just record for a fixed duration for simplicity.\n            duration = 5  # seconds\n            myrecording = sd.rec(int(duration * self.samplerate), samplerate=self.samplerate, channels=self.channels)\n            sd.wait()  # Wait until recording is finished\n            \n            self.get_logger().info('Recording finished.')\n            self.is_recording = False\n\n            # Save to a temporary file to pass to Whisper\n            temp_file = \"/tmp/temp_audio.wav\"\n            write(temp_file, self.samplerate, myrecording)\n\n            # Transcribe\n            self.get_logger().info('Transcribing...')\n            result = self.model.transcribe(temp_file)\n            transcribed_text = result[\"text\"]\n            self.get_logger().info(f'Transcription: \"{transcribed_text}\"')\n            \n            # Publish the transcription\n            msg = String()\n            msg.data = transcribed_text\n            self.transcription_publisher.publish(msg)\n            \n            response.success = True\n            response.message = transcribed_text\n        else:\n            response.success = False\n            response.message = \"Already recording.\"\n            \n        return response\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = WhisperBridge()\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,t.jsx)(n.h3,{id:"how-to-use-this-node",children:"How to Use this Node"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Run the Node"}),": ",(0,t.jsx)(n.code,{children:"ros2 run my_robot_vla whisper_ros_bridge.py"})]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Trigger a Recording"}),": From another terminal, call the service:","\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"ros2 service call /trigger_transcription std_srvs/srv/Trigger\n"})}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Speak"}),": Speak a command into your microphone within the 5-second window."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Observe the Output"}),": The node will log the transcription and publish it to the ",(0,t.jsx)(n.code,{children:"/voice_transcription"})," topic. You can listen to the topic with:","\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"ros2 topic echo /voice_transcription\n"})}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"from-text-to-action-the-command-parser",children:"From Text to Action: The Command Parser"}),"\n",(0,t.jsxs)(n.p,{children:["The next step in the pipeline is a simple command parser. This node subscribes to ",(0,t.jsx)(n.code,{children:"/voice_transcription"})," and acts upon the text. For our MVP, we can use simple keyword spotting."]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsxs)(n.strong,{children:["Conceptual ",(0,t.jsx)(n.code,{children:"command_parser.py"}),":"]})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom action_msgs.msg import GoalStatus\nfrom nav2_msgs.action import NavigateToPose\nfrom rclpy.action import ActionClient\n\nclass CommandParser(Node):\n    def __init__(self):\n        super().__init__(\'command_parser\')\n        self.subscription = self.create_subscription(\n            String,\n            \'/voice_transcription\',\n            self.listener_callback,\n            10)\n        # Action client to call Nav2\n        self._action_client = ActionClient(self, NavigateToPose, \'navigate_to_pose\')\n\n    def listener_callback(self, msg):\n        command = msg.data.lower()\n        self.get_logger().info(f\'Received command: "{command}"\')\n\n        # Simple keyword-based parsing\n        if "go to the kitchen" in command:\n            self.send_nav_goal("kitchen")\n        elif "go to the table" in command:\n            self.send_nav_goal("table")\n\n    def send_nav_goal(self, location_name):\n        self.get_logger().info(f"Sending navigation goal for \'{location_name}\'...")\n        goal_msg = NavigateToPose.Goal()\n        # In a real system, you would look up the coordinates for \'location_name\'\n        # from a database or parameter file.\n        if location_name == "kitchen":\n            goal_msg.pose.pose.position.x = 5.0\n            goal_msg.pose.pose.position.y = -2.0\n            goal_msg.pose.pose.orientation.w = 1.0\n        else: # table\n            goal_msg.pose.pose.position.x = -1.5\n            goal_msg.pose.pose.position.y = 3.0\n            goal_msg.pose.pose.orientation.w = 1.0\n\n        self._action_client.wait_for_server()\n        self._send_goal_future = self._action_client.send_goal_async(goal_msg)\n        # Here you could add feedback callbacks, etc.\n        self.get_logger().info("Goal sent to Nav2.")\n\n# ... main function ...\n'})}),"\n",(0,t.jsx)(n.p,{children:"This simple pipeline demonstrates the power of this architecture. We have successfully created a bridge from human speech all the way to a high-level navigation command, with each step being a modular, inspectable ROS 2 component. In the next chapter, we will replace this simple keyword parser with a much more powerful and flexible LLM-based planner."})]})}function h(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}}}]);